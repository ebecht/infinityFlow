<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Etienne Becht" />


<title>Training non default regression models</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />




</head>

<body>




<h1 class="title toc-ignore">Training non default regression models</h1>
<h4 class="author">Etienne Becht</h4>
<h4 class="date">June 2020</h4>



<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>This vignette explains how to specify non-default machine learning frameworks and their hyperparameters when applying Infinity Flow. We will assume here that the basic usage of Infinity Flow has already been read, if you are not familiar with this material I suggest you first look at the <a href="basic_usage.html">basic usage vignette</a></p>
<p>This vignette will cover:</p>
<ol style="list-style-type: decimal">
<li>Loading the example data</li>
<li>Note on package design</li>
<li>The <code>regression_functions</code> argument</li>
<li>The <code>extra_args_regression_params</code> argument</li>
<li>Neural networks</li>
</ol>
</div>
<div id="loading-the-example-data" class="section level1">
<h1>Loading the example data</h1>
<p>Here is a single R code chunk that recapitulates all of the data preparation covered in the <a href="basic_usage.html">basic usage vignette</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">if</span>(<span class="op">!</span><span class="kw">require</span>(devtools)){
    <span class="kw">install.packages</span>(<span class="st">&quot;devtools&quot;</span>)
}
<span class="cf">if</span>(<span class="op">!</span><span class="kw">require</span>(infinityFlow)){
    <span class="kw">library</span>(devtools)
    <span class="kw">install_github</span>(<span class="st">&quot;ebecht/infinityFlow&quot;</span>)
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(flowCore)
<span class="kw">library</span>(infinityFlow)

<span class="kw">data</span>(steady_state_lung)
<span class="kw">data</span>(steady_state_lung_annotation)
<span class="kw">data</span>(steady_state_lung_backbone_specification)

dir &lt;-<span class="st"> </span><span class="kw">file.path</span>(<span class="kw">tempdir</span>(), <span class="st">&quot;infinity_flow_example&quot;</span>)
input_dir &lt;-<span class="st"> </span><span class="kw">file.path</span>(dir, <span class="st">&quot;fcs&quot;</span>)
<span class="kw">write.flowSet</span>(steady_state_lung, <span class="dt">outdir =</span> input_dir)
<span class="co">#&gt; [1] &quot;/tmp/Rtmp5WBQl7/infinity_flow_example/fcs&quot;</span>

<span class="kw">write.csv</span>(steady_state_lung_backbone_specification, <span class="dt">file =</span> <span class="kw">file.path</span>(dir, <span class="st">&quot;backbone_selection_file.csv&quot;</span>), <span class="dt">row.names =</span> <span class="ot">FALSE</span>)

path_to_fcs &lt;-<span class="st"> </span><span class="kw">file.path</span>(dir, <span class="st">&quot;fcs&quot;</span>)
path_to_output &lt;-<span class="st"> </span><span class="kw">file.path</span>(dir, <span class="st">&quot;output&quot;</span>)
path_to_intermediary_results &lt;-<span class="st"> </span><span class="kw">file.path</span>(dir, <span class="st">&quot;tmp&quot;</span>)
backbone_selection_file &lt;-<span class="st"> </span><span class="kw">file.path</span>(dir, <span class="st">&quot;backbone_selection_file.csv&quot;</span>)

targets &lt;-<span class="st"> </span>steady_state_lung_annotation<span class="op">$</span>Infinity_target
<span class="kw">names</span>(targets) &lt;-<span class="st"> </span><span class="kw">rownames</span>(steady_state_lung_annotation)
isotypes &lt;-<span class="st"> </span>steady_state_lung_annotation<span class="op">$</span>Infinity_isotype
<span class="kw">names</span>(isotypes) &lt;-<span class="st"> </span><span class="kw">rownames</span>(steady_state_lung_annotation)

input_events_downsampling &lt;-<span class="st"> </span><span class="dv">1000</span>
prediction_events_downsampling &lt;-<span class="st"> </span><span class="dv">500</span>
cores =<span class="st"> </span>1L</code></pre></div>
</div>
<div id="note-on-package-design" class="section level1">
<h1>Note on package design</h1>
<p>The <code>infinity_flow()</code> function which encapsulates the complete Infinity Flow computational pipeline uses two arguments to respectively select regression models and their hyperparameters. These two arguments are both lists, and should have the same length. The idea is that the first list, <code>regression_functions</code> will be a list of model templates (XGBoost, Neural Networks, SVMs...) to train, while the second will be used to specify their hyperparameters. The list of templates is then fit to the data using parallel computing with socketing (using the <code>parallel</code> package through the <code>pbapply</code> package), which is more memory efficient.</p>
</div>
<div id="the-regression_functions-argument" class="section level1">
<h1>The <code>regression_functions</code> argument</h1>
<p>This argument is a list of functions which specifies how many models to train per well and which ones. Each type of machine learning model is supported through a wrapper in the <strong><em>infinityFlow</em></strong> package, and has a name of the form <code>fitter_*</code>. See below for the complete list:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(<span class="kw">grep</span>(<span class="st">&quot;fitter_&quot;</span>, <span class="kw">ls</span>(<span class="st">&quot;package:infinityFlow&quot;</span>), <span class="dt">value =</span> <span class="ot">TRUE</span>))
<span class="co">#&gt; [1] &quot;fitter_glmnet&quot;  &quot;fitter_linear&quot;  &quot;fitter_nn&quot;      &quot;fitter_svm&quot;    </span>
<span class="co">#&gt; [5] &quot;fitter_xgboost&quot;</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th>fitter_ function</th>
<th>Backend</th>
<th>Model type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>fitter_xgboost</td>
<td>XGBoost</td>
<td>Gradient boosted trees</td>
</tr>
<tr class="even">
<td>fitter_nn</td>
<td>Tensorflow/Keras</td>
<td>Neural networks</td>
</tr>
<tr class="odd">
<td>fitter_svm</td>
<td>e1071</td>
<td>Support vector machines</td>
</tr>
<tr class="even">
<td>fitter_glmnet</td>
<td>glmnet</td>
<td>Generalized linear and polynomial models</td>
</tr>
<tr class="odd">
<td>fitter_lm</td>
<td>stats</td>
<td>Linear and polynomial models</td>
</tr>
</tbody>
</table>
<p>These functions rely on optional package dependencies (so that you do not need to install e.g. Keras if you are not planning to use it). We need to make sure that these dependencies are however met:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">       optional_dependencies &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;glmnetUtils&quot;</span>, <span class="st">&quot;e1071&quot;</span>)
       unmet_dependencies &lt;-<span class="st"> </span><span class="kw">setdiff</span>(optional_dependencies, <span class="kw">rownames</span>(<span class="kw">installed.packages</span>()))
       <span class="cf">if</span>(<span class="kw">length</span>(unmet_dependencies) <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>){
           <span class="kw">install.packages</span>(unmet_dependencies)
       }
       <span class="cf">for</span>(pkg <span class="cf">in</span> optional_dependencies){
           <span class="kw">library</span>(pkg, <span class="dt">character.only =</span> <span class="ot">TRUE</span>)
       }</code></pre></div>
<p>In this vignette we will train all of these models. Note that if you do it on your own data, it make take quite a bit of memory (remember that the output expression matrix will be a numeric matrix of size <code>(prediction_events_downsampling x number of wells) rows x (number of wells x number of models)</code>.</p>
<p>To train multiple models we create a list of these fitter_* functions and assign this to the <code>regression_functions</code> argument that will be fed to the <code>infinity_flow</code> function. The names of this list will be used to name your models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">regression_functions &lt;-<span class="st"> </span><span class="kw">list</span>(
    <span class="dt">XGBoost =</span> fitter_xgboost, <span class="co"># XGBoost</span>
    <span class="dt">SVM =</span> fitter_svm, <span class="co"># SVM</span>
    <span class="dt">LASSO2 =</span> fitter_glmnet, <span class="co"># L1-penalized 2nd degree polynomial model</span>
    <span class="dt">LM =</span> fitter_linear <span class="co"># Linear model</span>
)</code></pre></div>
</div>
<div id="the-extra_args_regression_params-argument" class="section level1">
<h1>The <code>extra_args_regression_params</code> argument</h1>
<p>This argument is a list of list (so of the form <code>list(list(...), list(...), etc.)</code>) of length <code>length(regression_functions)</code>. Each element of the extra_args_regression_params object is thus a list. This lower-level list will be used to pass named arguments to the machine learning fitting function. The list of <code>extra_args_regression_params</code> is matched with the list of machine learning models <code>regression_functions</code> using the order of the elements in these two lists (e.g. the first regression model is matched with the first element of the list of arguments, then the seconds elements are matched together, etc...).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">backbone_size &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="kw">read.csv</span>(backbone_selection_file)[,<span class="st">&quot;type&quot;</span>])[<span class="st">&quot;backbone&quot;</span>]
extra_args_regression_params &lt;-<span class="st"> </span><span class="kw">list</span>(
     ## Passed to the first element of `regression_functions`, e.g. XGBoost. See ?xgboost for which parameters can be passed through this list
    <span class="kw">list</span>(<span class="dt">nrounds =</span> <span class="dv">500</span>, <span class="dt">eta =</span> <span class="fl">0.05</span>),

    <span class="co"># ## Passed to the second element of `regression_functions`, e.g. neural networks through keras::fit. See https://keras.rstudio.com/articles/tutorial_basic_regression.html</span>
    <span class="co"># list(</span>
    <span class="co">#         object = { ## Specifies the network's architecture, loss function and optimization method</span>
    <span class="co">#             model = keras_model_sequential()</span>
    <span class="co">#             model %&gt;%</span>
    <span class="co">#                 layer_dense(units = backbone_size, activation = &quot;relu&quot;, input_shape = backbone_size) %&gt;%</span>
    <span class="co">#                 layer_dense(units = backbone_size, activation = &quot;relu&quot;, input_shape = backbone_size) %&gt;%</span>
    <span class="co">#                 layer_dense(units = 1, activation = &quot;linear&quot;)</span>
    <span class="co">#             model %&gt;%</span>
    <span class="co">#                 compile(loss = &quot;mean_squared_error&quot;, optimizer = optimizer_sgd(lr = 0.005))</span>
    <span class="co">#             serialize_model(model)</span>
    <span class="co">#         },</span>
    <span class="co">#         epochs = 1000, ## Number of maximum training epochs. The training is however stopped early if the loss on the validation set does not improve for 20 epochs. This early stopping is hardcoded in fitter_nn.</span>
    <span class="co">#         validation_split = 0.2, ## Fraction of the training data used to monitor validation loss</span>
    <span class="co">#         verbose = 0,</span>
    <span class="co">#         batch_size = 128 ## Size of the minibatches for training.</span>
    <span class="co"># ),</span>

    <span class="co"># Passed to the third element, SVMs. See help(svm, &quot;e1071&quot;) for possible arguments</span>
    <span class="kw">list</span>(<span class="dt">type =</span> <span class="st">&quot;nu-regression&quot;</span>, <span class="dt">cost =</span> <span class="dv">8</span>, <span class="dt">nu=</span><span class="fl">0.5</span>, <span class="dt">kernel=</span><span class="st">&quot;radial&quot;</span>),

    <span class="co"># Passed to the fourth element, fitter_glmnet. This should contain a mandatory argument `degree` which specifies the degree of the polynomial model (1 for linear, 2 for quadratic etc...). Here we use degree = 2 corresponding to our LASSO2 model Other arguments are passed to getS3method(&quot;cv.glmnet&quot;, &quot;formula&quot;),</span>
    <span class="kw">list</span>(<span class="dt">alpha =</span> <span class="dv">1</span>, <span class="dt">nfolds=</span><span class="dv">10</span>, <span class="dt">degree =</span> <span class="dv">2</span>),

    <span class="co"># Passed to the fourth element, fitter_linear. This only accepts a degree argument specifying the degree of the polynomial model. Here we use degree = 1 corresponding to a linear model.</span>
    <span class="kw">list</span>(<span class="dt">degree =</span> <span class="dv">1</span>)
)</code></pre></div>
<p>We can now run the pipeline with these custom arguments to train all the models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">if</span>(<span class="kw">length</span>(regression_functions) <span class="op">!=</span><span class="st"> </span><span class="kw">length</span>(extra_args_regression_params)){
    <span class="kw">stop</span>(<span class="st">&quot;Number of models and number of lists of hyperparameters mismatch&quot;</span>)
}
imputed_data &lt;-<span class="st"> </span><span class="kw">infinity_flow</span>(
    <span class="dt">regression_functions =</span> regression_functions,
    <span class="dt">extra_args_regression_params =</span> extra_args_regression_params,
    <span class="dt">path_to_fcs =</span> path_to_fcs,
    <span class="dt">path_to_output =</span> path_to_output,
    <span class="dt">path_to_intermediary_results =</span> path_to_intermediary_results,
    <span class="dt">backbone_selection_file =</span> backbone_selection_file,
    <span class="dt">annotation =</span> targets,
    <span class="dt">isotype =</span> isotypes,
    <span class="dt">input_events_downsampling =</span> input_events_downsampling,
    <span class="dt">prediction_events_downsampling =</span> prediction_events_downsampling,
    <span class="dt">verbose =</span> <span class="ot">TRUE</span>,
    <span class="dt">cores =</span> cores
)
<span class="co">#&gt; Using directories...</span>
<span class="co">#&gt;  input: /tmp/Rtmp5WBQl7/infinity_flow_example/fcs</span>
<span class="co">#&gt;  intermediary: /tmp/Rtmp5WBQl7/infinity_flow_example/tmp</span>
<span class="co">#&gt;  subset: /tmp/Rtmp5WBQl7/infinity_flow_example/tmp/subsetted_fcs</span>
<span class="co">#&gt;  rds: /tmp/Rtmp5WBQl7/infinity_flow_example/tmp/rds</span>
<span class="co">#&gt;  annotation: /tmp/Rtmp5WBQl7/infinity_flow_example/tmp/annotation.csv</span>
<span class="co">#&gt;  output: /tmp/Rtmp5WBQl7/infinity_flow_example/output</span>
<span class="co">#&gt; Parsing and subsampling input data</span>
<span class="co">#&gt;  Downsampling to 1000 events per input file</span>
<span class="co">#&gt;  Concatenating expression matrices</span>
<span class="co">#&gt;  Writing to disk</span>
<span class="co">#&gt; Logicle-transforming the data</span>
<span class="co">#&gt;  Backbone data</span>
<span class="co">#&gt;  Exploratory data</span>
<span class="co">#&gt;  Writing to disk</span>
<span class="co">#&gt;  Transforming expression matrix</span>
<span class="co">#&gt;  Writing to disk</span>
<span class="co">#&gt; Harmonizing backbone data</span>
<span class="co">#&gt;  Scaling expression matrices</span>
<span class="co">#&gt;  Writing to disk</span>
<span class="co">#&gt; Fitting regression models</span>
<span class="co">#&gt;  Randomly selecting 50% of the subsetted input files to fit models</span>
<span class="co">#&gt;  Fitting...</span>
<span class="co">#&gt;      XGBoost</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  9.534229 seconds</span>
<span class="co">#&gt;      SVM</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  1.003363 seconds</span>
<span class="co">#&gt;      LASSO2</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  7.52319 seconds</span>
<span class="co">#&gt;      LM</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  0.07620192 seconds</span>
<span class="co">#&gt; Imputing missing measurements</span>
<span class="co">#&gt;  Randomly drawing events to predict from the test set</span>
<span class="co">#&gt;  Imputing...</span>
<span class="co">#&gt;      XGBoost</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  2.000606 seconds</span>
<span class="co">#&gt;      SVM</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  1.474077 seconds</span>
<span class="co">#&gt;      LASSO2</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  1.298271 seconds</span>
<span class="co">#&gt;      LM</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  0.06238985 seconds</span>
<span class="co">#&gt;  Concatenating predictions</span>
<span class="co">#&gt;  Writing to disk</span>
<span class="co">#&gt; Performing dimensionality reduction</span>
<span class="co">#&gt; 18:08:38 UMAP embedding parameters a = 1.262 b = 1.003</span>
<span class="co">#&gt; 18:08:38 Read 5000 rows and found 17 numeric columns</span>
<span class="co">#&gt; 18:08:38 Using Annoy for neighbor search, n_neighbors = 15</span>
<span class="co">#&gt; 18:08:38 Building Annoy index with metric = euclidean, n_trees = 50</span>
<span class="co">#&gt; 0%   10   20   30   40   50   60   70   80   90   100%</span>
<span class="co">#&gt; [----|----|----|----|----|----|----|----|----|----|</span>
<span class="co">#&gt; **************************************************|</span>
<span class="co">#&gt; 18:08:39 Writing NN index file to temp file /tmp/Rtmp5WBQl7/file5bea358a04f3</span>
<span class="co">#&gt; 18:08:39 Searching Annoy index using 1 thread, search_k = 1500</span>
<span class="co">#&gt; 18:08:40 Annoy recall = 100%</span>
<span class="co">#&gt; 18:08:40 Commencing smooth kNN distance calibration using 1 thread</span>
<span class="co">#&gt; 18:08:41 Initializing from normalized Laplacian + noise</span>
<span class="co">#&gt; 18:08:41 Commencing optimization for 1000 epochs, with 102010 positive edges using 1 thread</span>
<span class="co">#&gt; 18:08:52 Optimization finished</span>
<span class="co">#&gt; Exporting results</span>
<span class="co">#&gt;  Transforming predictions back to a linear scale</span>
<span class="co">#&gt;  Exporting FCS files (1 per well)</span>
<span class="co">#&gt; Plotting</span>
<span class="co">#&gt;  Chopping off the top and bottom 0.005 quantiles</span>
<span class="co">#&gt;  Shuffling the order of cells (rows)</span>
<span class="co">#&gt;  Producing plot</span>
<span class="co">#&gt; Background correcting</span>
<span class="co">#&gt;  Transforming background-corrected predictions. (Use logarithm to visualize)</span>
<span class="co">#&gt;  Exporting FCS files (1 per well)</span>
<span class="co">#&gt; Plotting</span>
<span class="co">#&gt;  Chopping off the top and bottom 0.005 quantiles</span>
<span class="co">#&gt;  Shuffling the order of cells (rows)</span>
<span class="co">#&gt;  Producing plot</span></code></pre></div>
<p>Our model names are appended to the predicted markers in the output. For more discussion about the outputs (including output files written to disk and plots), see the <a href="basic_usage.html">basic usage vignette</a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">   <span class="kw">print</span>(imputed_data<span class="op">$</span>bgc[<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>, ])
<span class="co">#&gt;      FSC-A      FSC-H     FSC-W   SSC-A      SSC-H      SSC-W CD69-CD301b</span>
<span class="co">#&gt; 3 32859.87 -2.1452424 1.3004626 8654.45  0.9182352  0.7844959   -2.063537</span>
<span class="co">#&gt; 5 43608.00 -0.7393602 0.2670576 3692.46 -0.1769030 -0.2410487    1.070597</span>
<span class="co">#&gt;      Zombie      MHCII        CD4      CD44        CD8     CD11c     CD11b</span>
<span class="co">#&gt; 3 299.31223  0.3458004 -2.2039795 1.1258929 -1.6157887 0.1735992 1.7460077</span>
<span class="co">#&gt; 5 -99.66527 -1.0175825 -0.9684353 0.1033653 -0.3845943 0.2564877 0.8890243</span>
<span class="co">#&gt;         F480       Ly6C    Lineage   CD45a488 FJComp-PE(yg)-A       CD24</span>
<span class="co">#&gt; 3 1.43461814  1.7668502 -2.6759187  0.7965220        1.129994  0.1660313</span>
<span class="co">#&gt; 5 0.02264125 -0.5782225 -0.9524925 -0.5375827        1.329581 -0.6896044</span>
<span class="co">#&gt;       CD103     Time CD137.LASSO2_bgc CD137.LM_bgc CD137.SVM_bgc</span>
<span class="co">#&gt; 3 1.6669930 2817.607       0.56318493    0.1090642     1.2227003</span>
<span class="co">#&gt; 5 0.2772005 2466.706       0.06101084    0.0575776    -0.2291964</span>
<span class="co">#&gt;   CD137.XGBoost_bgc CD28.LASSO2_bgc CD28.LM_bgc CD28.SVM_bgc CD28.XGBoost_bgc</span>
<span class="co">#&gt; 3        -0.3886328      0.07161657 -0.29364757  -0.09044011       -0.6625075</span>
<span class="co">#&gt; 5        -0.1397716      0.05399648  0.04782923   0.04084691        0.3005340</span>
<span class="co">#&gt;   CD49b(pan-NK).LASSO2_bgc CD49b(pan-NK).LM_bgc CD49b(pan-NK).SVM_bgc</span>
<span class="co">#&gt; 3               -0.1963101          -0.05667896            0.02219718</span>
<span class="co">#&gt; 5                0.4656097           0.16038101            0.61572513</span>
<span class="co">#&gt;   CD49b(pan-NK).XGBoost_bgc KLRG1.LASSO2_bgc KLRG1.LM_bgc KLRG1.SVM_bgc</span>
<span class="co">#&gt; 3               -0.23385492      0.002918804   -0.1280181    -0.1891422</span>
<span class="co">#&gt; 5                0.07923568      0.340717625    0.2626362    -0.7304185</span>
<span class="co">#&gt;   KLRG1.XGBoost_bgc Ly-49c/F/I/H.LASSO2_bgc Ly-49c/F/I/H.LM_bgc</span>
<span class="co">#&gt; 3        -0.6001133              0.59240144          -0.2111478</span>
<span class="co">#&gt; 5         0.4505542              0.05483269           0.1463673</span>
<span class="co">#&gt;   Ly-49c/F/I/H.SVM_bgc Ly-49c/F/I/H.XGBoost_bgc Podoplanin.LASSO2_bgc</span>
<span class="co">#&gt; 3            0.7580827               -0.9764462           -0.17150060</span>
<span class="co">#&gt; 5           -0.4994389                0.4167188           -0.01817265</span>
<span class="co">#&gt;   Podoplanin.LM_bgc Podoplanin.SVM_bgc Podoplanin.XGBoost_bgc rIgM.LASSO2_bgc</span>
<span class="co">#&gt; 3       -0.92895398         0.07758797             -0.1997421   -4.401225e-16</span>
<span class="co">#&gt; 5       -0.00719607        -0.69892311              0.3322251    3.449237e-16</span>
<span class="co">#&gt;    rIgM.LM_bgc  rIgM.SVM_bgc rIgM.XGBoost_bgc SHIgG.LASSO2_bgc  SHIgG.LM_bgc</span>
<span class="co">#&gt; 3 1.631903e-15 -1.650115e-16     6.760505e-17     5.524297e-16 -8.940419e-17</span>
<span class="co">#&gt; 5 5.328382e-16 -3.220208e-16    -8.940419e-17    -3.896258e-16  6.760505e-17</span>
<span class="co">#&gt;   SHIgG.SVM_bgc SHIgG.XGBoost_bgc SSEA-3.LASSO2_bgc SSEA-3.LM_bgc</span>
<span class="co">#&gt; 3 -3.700343e-16      1.210268e-15         0.9744609     0.2742498</span>
<span class="co">#&gt; 5 -6.840528e-16     -9.878614e-16        -0.1156342     0.1272408</span>
<span class="co">#&gt;   SSEA-3.SVM_bgc SSEA-3.XGBoost_bgc TCR Vg3.LASSO2_bgc TCR Vg3.LM_bgc</span>
<span class="co">#&gt; 3      1.1278812          0.5531739          0.2353065    -0.38597948</span>
<span class="co">#&gt; 5     -0.7584055         -0.2926596          0.1567185     0.09881684</span>
<span class="co">#&gt;   TCR Vg3.SVM_bgc TCR Vg3.XGBoost_bgc    UMAP1    UMAP2 PE_id</span>
<span class="co">#&gt; 3      -0.9841902          -0.6766347 599.6355 493.3159     1</span>
<span class="co">#&gt; 5       0.5267482           0.8629833 703.6625 520.1462     1</span></code></pre></div>
</div>
<div id="neural-networks" class="section level1">
<h1>Neural networks</h1>
<p>Neural networks won't build in knitr for me but here is an example of the syntax if you want to use them.</p>
<p>Note: there is an issue with serialization of the neural networks and socketing since I updated to R-4.0.1. If you want to use neural networks, please make sure to set</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cores =<span class="st"> </span>1L</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">optional_dependencies &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;keras&quot;</span>, <span class="st">&quot;tensorflow&quot;</span>)
unmet_dependencies &lt;-<span class="st"> </span><span class="kw">setdiff</span>(optional_dependencies, <span class="kw">rownames</span>(<span class="kw">installed.packages</span>()))
<span class="cf">if</span>(<span class="kw">length</span>(unmet_dependencies) <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>){
     <span class="kw">install.packages</span>(unmet_dependencies)
}
<span class="cf">for</span>(pkg <span class="cf">in</span> optional_dependencies){
    <span class="kw">library</span>(pkg, <span class="dt">character.only =</span> <span class="ot">TRUE</span>)
}

<span class="kw">invisible</span>(<span class="kw">eval</span>(<span class="kw">try</span>(<span class="kw">keras_model_sequential</span>()))) ## avoids conflicts with flowCore...

<span class="cf">if</span>(<span class="op">!</span><span class="kw">is_keras_available</span>()){
     <span class="kw">install_keras</span>() ## Instal keras unsing the R interface - can take a while
}

<span class="cf">if</span>(<span class="op">!</span><span class="kw">require</span>(devtools)){
    <span class="kw">install.packages</span>(<span class="st">&quot;devtools&quot;</span>)
}
<span class="cf">if</span>(<span class="op">!</span><span class="kw">require</span>(infinityFlow)){
    <span class="kw">library</span>(devtools)
    <span class="kw">install_github</span>(<span class="st">&quot;ebecht/infinityFlow&quot;</span>)
}
<span class="kw">library</span>(flowCore)
<span class="kw">library</span>(infinityFlow)

<span class="kw">data</span>(steady_state_lung)
<span class="kw">data</span>(steady_state_lung_annotation)
<span class="kw">data</span>(steady_state_lung_backbone_specification)

dir &lt;-<span class="st"> </span><span class="kw">file.path</span>(<span class="kw">tempdir</span>(), <span class="st">&quot;infinity_flow_example&quot;</span>)
input_dir &lt;-<span class="st"> </span><span class="kw">file.path</span>(dir, <span class="st">&quot;fcs&quot;</span>)
<span class="kw">write.flowSet</span>(steady_state_lung, <span class="dt">outdir =</span> input_dir)

<span class="kw">write.csv</span>(steady_state_lung_backbone_specification, <span class="dt">file =</span> <span class="kw">file.path</span>(dir, <span class="st">&quot;backbone_selection_file.csv&quot;</span>), <span class="dt">row.names =</span> <span class="ot">FALSE</span>)

path_to_fcs &lt;-<span class="st"> </span><span class="kw">file.path</span>(dir, <span class="st">&quot;fcs&quot;</span>)
path_to_output &lt;-<span class="st"> </span><span class="kw">file.path</span>(dir, <span class="st">&quot;output&quot;</span>)
path_to_intermediary_results &lt;-<span class="st"> </span><span class="kw">file.path</span>(dir, <span class="st">&quot;tmp&quot;</span>)
backbone_selection_file &lt;-<span class="st"> </span><span class="kw">file.path</span>(dir, <span class="st">&quot;backbone_selection_file.csv&quot;</span>)

targets &lt;-<span class="st"> </span>steady_state_lung_annotation<span class="op">$</span>Infinity_target
<span class="kw">names</span>(targets) &lt;-<span class="st"> </span><span class="kw">rownames</span>(steady_state_lung_annotation)
isotypes &lt;-<span class="st"> </span>steady_state_lung_annotation<span class="op">$</span>Infinity_isotype
<span class="kw">names</span>(isotypes) &lt;-<span class="st"> </span><span class="kw">rownames</span>(steady_state_lung_annotation)

input_events_downsampling &lt;-<span class="st"> </span><span class="dv">1000</span>
prediction_events_downsampling &lt;-<span class="st"> </span><span class="dv">500</span>

## Passed to fitter_nn, e.g. neural networks through keras::fit. See https://keras.rstudio.com/articles/tutorial_basic_regression.html
regression_functions &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">NN =</span> fitter_nn)

backbone_size &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="kw">read.csv</span>(backbone_selection_file)[,<span class="st">&quot;type&quot;</span>])[<span class="st">&quot;backbone&quot;</span>]
extra_args_regression_params &lt;-<span class="st"> </span><span class="kw">list</span>(
        <span class="kw">list</span>(
        <span class="dt">object =</span> { ## Specifies the network's architecture, loss function and optimization method
        model =<span class="st"> </span><span class="kw">keras_model_sequential</span>()
        model <span class="op">%&gt;%</span>
<span class="st">        </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> backbone_size, <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="dt">input_shape =</span> backbone_size) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">        </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> backbone_size, <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>, <span class="dt">input_shape =</span> backbone_size) <span class="op">%&gt;%</span>
<span class="st">        </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">1</span>, <span class="dt">activation =</span> <span class="st">&quot;linear&quot;</span>)
        model <span class="op">%&gt;%</span>
<span class="st">        </span><span class="kw">compile</span>(<span class="dt">loss =</span> <span class="st">&quot;mean_squared_error&quot;</span>, <span class="dt">optimizer =</span> <span class="kw">optimizer_sgd</span>(<span class="dt">lr =</span> <span class="fl">0.005</span>))
        <span class="kw">serialize_model</span>(model)
        },
        <span class="dt">epochs =</span> <span class="dv">1000</span>, ## Number of maximum training epochs. The training is however stopped early if the loss on the validation set does not improve for 20 epochs. This early stopping is hardcoded in fitter_nn.
        <span class="dt">validation_split =</span> <span class="fl">0.2</span>, ## Fraction of the training data used to monitor validation loss
        <span class="dt">verbose =</span> <span class="dv">0</span>,
        <span class="dt">batch_size =</span> <span class="dv">128</span> ## Size of the minibatches for training.
    )
)

imputed_data &lt;-<span class="st"> </span><span class="kw">infinity_flow</span>(
    <span class="dt">regression_functions =</span> regression_functions,
    <span class="dt">extra_args_regression_params =</span> extra_args_regression_params,
    <span class="dt">path_to_fcs =</span> path_to_fcs,
    <span class="dt">path_to_output =</span> path_to_output,
    <span class="dt">path_to_intermediary_results =</span> path_to_intermediary_results,
    <span class="dt">backbone_selection_file =</span> backbone_selection_file,
    <span class="dt">annotation =</span> targets,
    <span class="dt">isotype =</span> isotypes,
    <span class="dt">input_events_downsampling =</span> input_events_downsampling,
    <span class="dt">prediction_events_downsampling =</span> prediction_events_downsampling,
    <span class="dt">verbose =</span> <span class="ot">TRUE</span>,
    <span class="dt">cores =</span> 1L
)</code></pre></div>
</div>
<div id="information-about-the-r-session-when-this-vignette-was-built" class="section level1">
<h1>Information about the R session when this vignette was built</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sessionInfo</span>()
<span class="co">#&gt; R version 4.0.1 (2020-06-06)</span>
<span class="co">#&gt; Platform: x86_64-pc-linux-gnu (64-bit)</span>
<span class="co">#&gt; Running under: Ubuntu 18.04.4 LTS</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Matrix products: default</span>
<span class="co">#&gt; BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1</span>
<span class="co">#&gt; LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Random number generation:</span>
<span class="co">#&gt;  RNG:     L'Ecuyer-CMRG </span>
<span class="co">#&gt;  Normal:  Inversion </span>
<span class="co">#&gt;  Sample:  Rejection </span>
<span class="co">#&gt;  </span>
<span class="co">#&gt; locale:</span>
<span class="co">#&gt;  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              </span>
<span class="co">#&gt;  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    </span>
<span class="co">#&gt;  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   </span>
<span class="co">#&gt;  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 </span>
<span class="co">#&gt;  [9] LC_ADDRESS=C               LC_TELEPHONE=C            </span>
<span class="co">#&gt; [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; attached base packages:</span>
<span class="co">#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; other attached packages:</span>
<span class="co">#&gt; [1] e1071_1.7-3         glmnetUtils_1.1.5   infinityFlow_0.99.5</span>
<span class="co">#&gt; [4] flowCore_2.0.1     </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; loaded via a namespace (and not attached):</span>
<span class="co">#&gt;  [1] Rcpp_1.0.5          RSpectra_0.16-0     compiler_4.0.1     </span>
<span class="co">#&gt;  [4] cytolib_2.0.3       class_7.3-17        iterators_1.0.12   </span>
<span class="co">#&gt;  [7] tools_4.0.1         uwot_0.1.8          digest_0.6.25      </span>
<span class="co">#&gt; [10] evaluate_0.14       lattice_0.20-41     png_0.1-7          </span>
<span class="co">#&gt; [13] rlang_0.4.6         foreach_1.5.0       Matrix_1.2-18      </span>
<span class="co">#&gt; [16] yaml_2.2.1          parallel_4.0.1      xfun_0.15          </span>
<span class="co">#&gt; [19] stringr_1.4.0       knitr_1.29          raster_3.3-7       </span>
<span class="co">#&gt; [22] gtools_3.8.2        generics_0.0.2      stats4_4.0.1       </span>
<span class="co">#&gt; [25] xgboost_1.1.1.1     grid_4.0.1          glmnet_4.0-2       </span>
<span class="co">#&gt; [28] Biobase_2.48.0      data.table_1.12.8   RcppAnnoy_0.0.16   </span>
<span class="co">#&gt; [31] pbapply_1.4-2       survival_3.1-12     rmarkdown_2.3      </span>
<span class="co">#&gt; [34] sp_1.4-2            RProtoBufLib_2.0.0  matlab_1.0.2       </span>
<span class="co">#&gt; [37] magrittr_1.5        splines_4.0.1       codetools_0.2-16   </span>
<span class="co">#&gt; [40] matrixStats_0.56.0  htmltools_0.5.0     BiocGenerics_0.34.0</span>
<span class="co">#&gt; [43] shape_1.4.4         stringi_1.4.6       RcppParallel_5.0.2</span></code></pre></div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
